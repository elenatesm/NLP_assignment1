{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the context-sensitive spelling correction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bigrams(filename):\n",
    "    bigrams = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            freq, word1, word2 = line.strip().split()\n",
    "            bigrams[(word1, word2)] = int(freq)\n",
    "    return bigrams\n",
    "\n",
    "def load_fivegrams(filename):\n",
    "    fivegrams = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            freq = int(values[0])\n",
    "            words = values[1:]\n",
    "            fivegrams[tuple(words)] = freq\n",
    "    return fivegrams\n",
    "\n",
    "def load_coca_links(filename):\n",
    "    coca_links = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            freq = int(values[0])\n",
    "            word = \" \".join(values[1:-2])\n",
    "            pos1 = values[-2]\n",
    "            pos2 = values[-1]\n",
    "            coca_links[word] = {'freq': freq, 'pos1': pos1, 'pos2': pos2}\n",
    "    return coca_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def context_sensitive_correction_word(word, context, bigrams, fivegrams, coca_links, WORDS):\n",
    "    # Calculate the Levenshtein distance between two words\n",
    "    def levenshtein_distance(word1, word2):\n",
    "        m = len(word1) + 1\n",
    "        n = len(word2) + 1\n",
    "        d = [[0] * n for _ in range(m)]\n",
    "        for i in range(1, m):\n",
    "            d[i][0] = i\n",
    "        for j in range(1, n):\n",
    "            d[0][j] = j\n",
    "        for i in range(1, m):\n",
    "            for j in range(1, n):\n",
    "                cost = 0 if word1[i-1] == word2[j-1] else 1\n",
    "                d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + cost)\n",
    "        return d[m-1][n-1]\n",
    "\n",
    "    # Extract features from a word\n",
    "    def get_word_features(word):\n",
    "        features = []\n",
    "        features.append(len(word)) # Word length\n",
    "        features.append(word[0]) # First character\n",
    "        features.append(word[-1]) # Last character\n",
    "        features.append(sum([c in 'aeiou' for c in word]))  # Vowel count\n",
    "        features.append(sum([c in 'bcdfghjklmnpqrstvwxyz' for c in word]))  # Consonant count\n",
    "        return features\n",
    "\n",
    "    # Perform correction of a single word\n",
    "    def get_correction(word, context, bigrams, fivegrams, coca_links, WORDS):\n",
    "        corrections = []\n",
    "        for dict_word in WORDS:\n",
    "            distance = levenshtein_distance(word, dict_word)\n",
    "            if distance <= 2:\n",
    "                word_features = get_word_features(word)\n",
    "                dict_word_features = get_word_features(dict_word)\n",
    "                feature_distance = sum([a!= b for a, b in zip(word_features, dict_word_features)])\n",
    "                corrections.append((dict_word, distance, feature_distance))\n",
    "        if corrections:  # Проверка на пустоту списка\n",
    "            corrections.sort(key=lambda x: (x[1], x[2])) # Sort corrections by distance and feature distance\n",
    "            return corrections[0][0] # Return the most likely correction\n",
    "        else:\n",
    "            return word  # Return the original word if no corrections found\n",
    "\n",
    "    # Perform context-sensitive correction of a single word\n",
    "    def get_context_correction(word, context, bigrams, fivegrams, coca_links, WORDS):\n",
    "        corrections = []\n",
    "        for dict_word in WORDS:\n",
    "            distance = levenshtein_distance(word, dict_word)\n",
    "            if distance <= 2:\n",
    "                context_distance = 0\n",
    "                if context is not None:\n",
    "                    for i in range(len(context)):\n",
    "                        if context[i] == dict_word:\n",
    "                            context_distance += 1\n",
    "                word_features = get_word_features(word)\n",
    "                dict_word_features = get_word_features(dict_word)\n",
    "                feature_distance = sum([a!= b for a, b in zip(word_features, dict_word_features)])\n",
    "                corrections.append((dict_word, distance, context_distance, feature_distance))\n",
    "        if corrections:\n",
    "            corrections.sort(key=lambda x: (x[1], -x[2], x[3])) # Sort corrections by distance, context distance, feature distanc\n",
    "            return corrections[0][0] # Return the most likely correction\n",
    "        else:\n",
    "            return word  # Return the original word if no corrections found\n",
    "\n",
    "    if context is not None and len(context) > 0:\n",
    "        return get_context_correction(word, context, bigrams, fivegrams, coca_links, WORDS)\n",
    "    else:\n",
    "        return get_correction(word, context, bigrams, fivegrams, coca_links, WORDS)\n",
    "\n",
    "def context_sensitive_correction(text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        context = words[i-1] if i > 0 else None\n",
    "        corrected_word = context_sensitive_correction_word(word, context, bigrams, fivegrams, coca_links, WORDS)\n",
    "        if corrected_word is None:\n",
    "            corrected_word = word  \n",
    "        corrected_words.append(corrected_word)\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "def neural_network_correction(text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for i, word in enumerate(words):\n",
    "        context = words[i-1] if i > 0 else None\n",
    "        # Create a TF-IDF vectorizer to transform the word into a numerical representation\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform([word])\n",
    "        y = [word]\n",
    "        # Train a random forest classifier to predict the corrected word\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X, y)\n",
    "        corrected_word = clf.predict(vectorizer.transform([word]))[0]\n",
    "        corrected_words.append(corrected_word)\n",
    "    return' '.join(corrected_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norvig's Solution\n",
    "\n",
    "The implementation is taken from the website https://norvig.com/spell-correct.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def norvig_correction(text):\n",
    "    words = text.split()\n",
    "    corrected_words = [correction(word) for word in words]\n",
    "    return' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your decisions\n",
    "\n",
    "**Using Levenshtein distance:** The Levenshtein distance is used to calculate the distance between two words. This choice is based on the assumption that the edit distance between two words is a good indicator of their similarity. The Levenshtein distance is a common metric used in spell correction tasks.\n",
    "\n",
    "\n",
    "**Extracting word features:** The get_word_features function extracts features from a word, including its length, first character, last character, vowel count, and consonant count. These features are used to calculate the similarity between words. This choice is based on the assumption that these features are relevant to the task of spell correction.\n",
    "\n",
    "\n",
    "**Using a threshold for Levenshtein distance:** The code uses a threshold of 2 for the Levenshtein distance. This means that only words with a Levenshtein distance of 2 or less are considered as possible corrections. This choice is based on the assumption that words with a higher Levenshtein distance are less likely to be correct.\n",
    "\n",
    "\n",
    "**Using a context-sensitive approach:** The get_context_correction function takes into account the context in which a word is used. This choice is based on the assumption that the context can provide valuable information about the correct spelling of a word.\n",
    "\n",
    "\n",
    "**Using a random forest classifier:** The neural_network_correction function uses a random forest classifier to predict the corrected word. This choice is based on the assumption that a random forest classifier is a robust and accurate model for this task.\n",
    "\n",
    "\n",
    "**Using TF-IDF vectorization:** The neural_network_correction function uses TF-IDF vectorization to transform the word into a numerical representation. This choice is based on the assumption that TF-IDF is a suitable method for representing text data.\n",
    "\n",
    "\n",
    "**Training a model for each word:** The neural_network_correction function trains a separate model for each word. This choice is based on the assumption that each word requires a separate model to accurately predict its correction.\n",
    "\n",
    "\n",
    "**Using a simple threshold for context distance:** The get_context_correction function uses a simple threshold for context distance. This means that only words with a context distance of 1 or more are considered as possible corrections. This choice is based on the assumption that a higher context distance indicates a lower likelihood of correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your text here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = load_bigrams('bigrams.txt')\n",
    "fivegrams = load_fivegrams('fivegrams.txt')\n",
    "coca_links = load_coca_links('coca_all_links.txt')\n",
    "\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "text = open('big.txt').read()\n",
    "WORDS = Counter(words(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise: 0.1, My implementation accuracy: 0.93, Norvig implementation: 0.6\n",
      "Noise: 0.2, My implementation accuracy: 0.84, Norvig implementation: 0.57\n",
      "Noise: 0.3, My implementation accuracy: 0.71, Norvig implementation: 0.6\n",
      "Noise: 0.4, My implementation accuracy: 0.53, Norvig implementation: 0.56\n",
      "Noise: 0.5, My implementation accuracy: 0.53, Norvig implementation: 0.54\n"
     ]
    }
   ],
   "source": [
    "# Generate a test set with noise\n",
    "def generate_test_set(text, noise_probability):\n",
    "    words = text.split()\n",
    "    noisy_words = []\n",
    "    for word in words:\n",
    "        # Add noise into the word with a certain probability\n",
    "        if random.random() < noise_probability:\n",
    "            word = introduce_noise(word)\n",
    "        noisy_words.append(word)\n",
    "    return' '.join(noisy_words)\n",
    "\n",
    "# Add noise into a word\n",
    "def introduce_noise(word):\n",
    "    index = random.randint(0, len(word) - 1)\n",
    "    new_word = list(word)\n",
    "    # Replace the character at the selected index with a random letter\n",
    "    new_word[index] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "    return ''.join(new_word)\n",
    "\n",
    "# Evaluate the accuracy of a correction model\n",
    "def evaluate_correction(original_text, corrected_text):\n",
    "    words = original_text.split()\n",
    "    corrected_words = corrected_text.split()\n",
    "    correct_count = 0\n",
    "    min_len = min(len(words), len(corrected_words))\n",
    "    # Iterate over the words and count the correct ones\n",
    "    for i in range(min_len):\n",
    "        if words[i] == corrected_words[i]:\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count / len(words)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Function to test correction models\n",
    "def test_correction_models(text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    noise_probabilities = [0.1, 0.2, 0.3, 0.4, 0.5]  \n",
    "    # Сan be changed to have bigger test set\n",
    "    max_words = 100\n",
    "    for noise_probability in noise_probabilities:\n",
    "        noisy_text = generate_test_set(text, noise_probability)\n",
    "        words = noisy_text.split()[:max_words]\n",
    "        noisy_text =' '.join(words)\n",
    "        \n",
    "        # Saving the noisy text to a file\n",
    "        # with open(f\"noisy_text_{noise_probability}.txt\", \"w\") as f:\n",
    "        #     f.write(noisy_text)\n",
    "        \n",
    "        # Apply the correction models\n",
    "        my_model_text = neural_network_correction(noisy_text, bigrams, fivegrams, coca_links, WORDS)\n",
    "        norvig_model_text = norvig_correction(noisy_text)\n",
    "        my_accuracy = evaluate_correction(' '.join(text.split()[:max_words]), my_model_text)\n",
    "        norvig_accuracy = evaluate_correction(' '.join(text.split()[:max_words]), norvig_model_text)\n",
    "\n",
    "        print(f'Noise: {noise_probability}, My implementation accuracy: {my_accuracy}, Norvig implementation: {norvig_accuracy}')\n",
    "\n",
    "def save_results(noise_probability, original_text, noisy_text, my_model_text, norvig_model_text, my_accuracy, norvig_accuracy):\n",
    "    with open('results.txt', 'a') as f:\n",
    "        f.write(f'Noise: {noise_probability}\\n')\n",
    "        f.write(f'Original text: {original_text}\\n')\n",
    "        f.write(f'Noisy text: {noisy_text}\\n')\n",
    "        f.write(f'My implementation: {my_model_text}\\n')\n",
    "        f.write(f'Norvig implementation: {norvig_model_text}\\n')\n",
    "        f.write(f'My implementation accuracy: {my_accuracy}\\n')\n",
    "        f.write(f'Norvig implementation accuracy: {norvig_accuracy}\\n\\n')\n",
    "\n",
    "# Test correction models and save the results\n",
    "def test_correction_models_with_saving(text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    noise_probabilities = [0.1, 0.2, 0.3, 0.4, 0.5]  \n",
    "    # Сan be changed to have bigger test set\n",
    "    max_words = 100\n",
    "    for noise_probability in noise_probabilities:\n",
    "        noisy_text = generate_test_set(text, noise_probability)\n",
    "        words = noisy_text.split()[:max_words]\n",
    "        noisy_text =' '.join(words)\n",
    "        \n",
    "        my_model_text = neural_network_correction(noisy_text, bigrams, fivegrams, coca_links, WORDS)\n",
    "        norvig_model_text = norvig_correction(noisy_text)\n",
    "        my_accuracy = evaluate_correction(' '.join(text.split()[:max_words]), my_model_text)\n",
    "        norvig_accuracy = evaluate_correction(' '.join(text.split()[:max_words]), norvig_model_text)\n",
    "\n",
    "        print(f'Noise: {noise_probability}, My implementation accuracy: {my_accuracy}, Norvig implementation: {norvig_accuracy}')\n",
    "        save_results(noise_probability,' '.join(text.split()[:max_words]), noisy_text, my_model_text, norvig_model_text, my_accuracy, norvig_accuracy)\n",
    "\n",
    "#Run the test\n",
    "test_correction_models_with_saving(text, bigrams, fivegrams, coca_links, WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test correctors on the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "1dking sport\n",
      "\n",
      "Text with spelling corrections (My implementation):\n",
      "taking sport\n",
      "\n",
      "Text with spelling corrections (Norvig's implementation):\n",
      "taking sport\n"
     ]
    }
   ],
   "source": [
    "def test_correction_models(text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    my_model_text = context_sensitive_correction(text, bigrams, fivegrams, coca_links, WORDS)\n",
    "    norvig_model_text = norvig_correction(text)\n",
    "    print(\"Input text:\")\n",
    "    print(text)\n",
    "    print(\"\\nText with spelling corrections (My implementation):\")\n",
    "    print(my_model_text)\n",
    "    print(\"\\nText with spelling corrections (Norvig's implementation):\")\n",
    "    print(norvig_model_text)\n",
    "\n",
    "def check_correction_models(input_text, bigrams, fivegrams, coca_links, WORDS):\n",
    "    my_model_text = context_sensitive_correction(input_text, bigrams, fivegrams, coca_links, WORDS)\n",
    "    norvig_model_text = norvig_correction(input_text)\n",
    "    print(\"Input text:\") \n",
    "    print(input_text)\n",
    "    print(\"\\nText with spelling corrections (My implementation):\")\n",
    "    print(my_model_text)\n",
    "    print(\"\\nText with spelling corrections (Norvig's implementation):\")\n",
    "    print(norvig_model_text)\n",
    "    \n",
    "\n",
    "input_text = \"1dking sport\"\n",
    "check_correction_models(input_text, bigrams, fivegrams, coca_links, WORDS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
